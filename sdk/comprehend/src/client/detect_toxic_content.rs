// Code generated by software.amazon.smithy.rust.codegen.smithy-rs. DO NOT EDIT.
impl super::Client {
    /// Constructs a fluent builder for the [`DetectToxicContent`](crate::operation::detect_toxic_content::builders::DetectToxicContentFluentBuilder) operation.
                            ///
                            /// - The fluent builder is configurable:
    ///   - [`text_segments(TextSegment)`](crate::operation::detect_toxic_content::builders::DetectToxicContentFluentBuilder::text_segments) / [`set_text_segments(Option<Vec::<TextSegment>>)`](crate::operation::detect_toxic_content::builders::DetectToxicContentFluentBuilder::set_text_segments):<br>required: **true**<br><p>A list of up to 10 text strings. The maximum size for the list is 10 KB.</p><br>
    ///   - [`language_code(LanguageCode)`](crate::operation::detect_toxic_content::builders::DetectToxicContentFluentBuilder::language_code) / [`set_language_code(Option<LanguageCode>)`](crate::operation::detect_toxic_content::builders::DetectToxicContentFluentBuilder::set_language_code):<br>required: **true**<br><p>The language of the input text. Currently, English is the only supported language.</p><br>
                            /// - On success, responds with [`DetectToxicContentOutput`](crate::operation::detect_toxic_content::DetectToxicContentOutput) with field(s):
    ///   - [`result_list(Option<Vec::<ToxicLabels>>)`](crate::operation::detect_toxic_content::DetectToxicContentOutput::result_list): <p>Results of the content moderation analysis. Each entry in the results list contains a list of toxic content types identified in the text, along with a confidence score for each content type. The results list also includes a toxicity score for each entry in the results list. </p>
                            /// - On failure, responds with [`SdkError<DetectToxicContentError>`](crate::operation::detect_toxic_content::DetectToxicContentError)
    pub fn detect_toxic_content(&self) -> crate::operation::detect_toxic_content::builders::DetectToxicContentFluentBuilder {
                                crate::operation::detect_toxic_content::builders::DetectToxicContentFluentBuilder::new(self.handle.clone())
                            }
}

